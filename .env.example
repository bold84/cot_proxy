# Host port to expose the proxy service on
HOST_PORT=3000

# Target API endpoint for the proxy
TARGET_BASE_URL=http://your-model-server:8080/

# Enable debug logging (true/false)
DEBUG=false

# Timeout for API requests in seconds
API_REQUEST_TIMEOUT=3000

# LLM parameter overrides.
# Format: "model=MODEL_NAME,param1=val1,param2=val2;model=ANOTHER_MODEL,param3=val3"
# Special parameters for think tag stripping: think_tag_start, think_tag_end
# Example:
LLM_PARAMS=model=default,temperature=0.7,think_tag_start=<default_think>,think_tag_end=</default_think>
# Example for Qwen3 with specific think tags (ensure proper escaping if needed for shell, but raw for .env):
# LLM_PARAMS=model=hf.co/unsloth/Qwen3-14B-GGUF:Q6_K_XL,think_tag_start=\u003cthink\u003e,think_tag_end=\u003c/think\u003e\n\n

# Optional: Global default start tag for stripping thought processes.
# Overridden by model-specific think_tag_start in LLM_PARAMS. Defaults to '<think>' if not set.
THINK_TAG=<think>

# Optional: Global default end tag for stripping thought processes.
# Overridden by model-specific think_tag_end in LLM_PARAMS. Defaults to '</think>' if not set.
THINK_END_TAG=</think>